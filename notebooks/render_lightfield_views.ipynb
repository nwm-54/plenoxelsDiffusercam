{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e4f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a model that serves as our ground truth\n",
    "# for static scenes use splatfacto? probably better to use original 3dgs method so render function is the same\n",
    "# for dynamic scenes use any one of the 3dgs methods \n",
    "\n",
    "# render the images using custom viewpoints derived from the original viewpoint\n",
    "# vary the distance between the original training data and the object - can do this by maybe calculating the center of gravity of the object and a vector between that and the camera location. move translation along this vector?\n",
    "# is there a smarter way to more realistically simulate the level of multiplexing?\n",
    "\n",
    "# for dynamics, render lightfield views for each timestep\n",
    "# we are just comparing monocular video, still video?\n",
    "# we could also do multiview with 3 views...is there a 360 degree dataset\n",
    "# i guess we could just do our own rendering by training with our own custom dataset?\n",
    "# is it worth figuring out how to get the 3d model so we can render it for real with as much detail as possible (probably not)\n",
    "\n",
    "# from the train transforms.json read in the transforms and then generate new camera views\n",
    "# do i need to change the rotation if i translate the object along the vector? could change the rotation so that the camera is facing the object center of gravity. i wonder how much it’s deviating right now\n",
    "\n",
    "# instead of translating the object along the vector to cog i should just translate it along the primary view direction (which i should be able to get from the extrinsics)\n",
    "# maybe that’s a good way to generate unseen views anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868541de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change working directory if necessary\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "from argparse import ArgumentParser\n",
    "from typing import List\n",
    "\n",
    "os.chdir('/home/wl757/gaussian-splatting')\n",
    "print(os.getcwd())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from arguments import ModelParams, PipelineParams\n",
    "from argparse import Namespace\n",
    "from gaussian_renderer import render\n",
    "from scene.gaussian_model import GaussianModel\n",
    "from scene import Scene\n",
    "from scene.cameras import Camera\n",
    "from utils.general_utils import safe_state\n",
    "from utils.graphics_utils import getWorld2View2\n",
    "from utils.image_utils import psnr\n",
    "\n",
    "def get_combined_args(parser: ArgumentParser, cmdlne_string = List[str]):\n",
    "    cfgfile_string = \"Namespace()\"\n",
    "    args_cmdline = parser.parse_args(cmdlne_string)\n",
    "\n",
    "    try:\n",
    "        cfgfilepath = os.path.join(args_cmdline.model_path, \"cfg_args\")\n",
    "        print(\"Looking for config file in\", cfgfilepath)\n",
    "        with open(cfgfilepath) as cfg_file:\n",
    "            print(\"Config file found: {}\".format(cfgfilepath))\n",
    "            cfgfile_string = cfg_file.read()\n",
    "    except TypeError:\n",
    "        print(\"Config file not found at\")\n",
    "        pass\n",
    "    args_cfgfile = eval(cfgfile_string)\n",
    "\n",
    "    merged_dict = vars(args_cfgfile).copy()\n",
    "    print(merged_dict)\n",
    "    for k,v in vars(args_cmdline).items():\n",
    "        if v != None:\n",
    "            merged_dict[k] = v\n",
    "    return Namespace(**merged_dict)\n",
    "\n",
    "parser = ArgumentParser()\n",
    "model = ModelParams(parser, sentinel=True)\n",
    "pipeline = PipelineParams(parser)\n",
    "parser.add_argument(\"--iteration\", default=-1, type=int)\n",
    "parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Suppress output messages\")\n",
    "args = get_combined_args(parser, [\"--model_path\", \"/home/wl757/gaussian-splatting/output/lego\"])\n",
    "print(\"Rendering \" + args.model_path)\n",
    "\n",
    "# Initialize system state (RNG)\n",
    "safe_state(args.quiet)\n",
    "dataset = model.extract(args)\n",
    "iteration = args.iteration\n",
    "pp = pipeline.extract(args)\n",
    "\n",
    "with torch.no_grad():\n",
    "    gaussians = GaussianModel(dataset.sh_degree)\n",
    "    scene = Scene(dataset, gaussians, load_iteration=iteration, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c85f48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def center_of_gravity(gaussians: GaussianModel):\n",
    "    return gaussians.get_xyz.mean(dim=0)\n",
    "\n",
    "def camera_forward(camera: Camera):\n",
    "    R = camera.R\n",
    "    if not torch.is_tensor(R):\n",
    "        R = torch.tensor(R, dtype=torch.float32, device=camera.data_device)\n",
    "    \n",
    "    z_axis_cam = torch.tensor([0, 0, 1], dtype=torch.float32, device=camera.data_device)\n",
    "    forward = torch.mv(R, z_axis_cam)\n",
    "    forward = forward / torch.norm(forward)\n",
    "    return forward\n",
    "\n",
    "@torch.no_grad()\n",
    "def camera_to_object(camera: Camera, gaussians: GaussianModel):\n",
    "    cog = center_of_gravity(gaussians)\n",
    "    print(f\"cog: {cog.tolist()}\")\n",
    "    camera_position = camera.camera_center.to(cog.device)\n",
    "    camera_to_object_vector = cog - camera_position\n",
    "    distance = torch.norm(camera_to_object_vector)\n",
    "    vector = camera_to_object_vector / distance\n",
    "\n",
    "    return distance.item(), vector\n",
    "\n",
    "def translate_camera(original_camera: Camera, delta: List[float]):\n",
    "    new_camera: Camera = copy.deepcopy(original_camera)\n",
    "    delta = np.array(delta, dtype=np.float32)\n",
    "\n",
    "    # calculate new T (don't use camera.trans as it is not supported in other code)\n",
    "    new_world_center = new_camera.camera_center.cpu().numpy() + delta\n",
    "    new_camera_center = new_world_center / new_camera.scale - new_camera.trans\n",
    "    new_camera.T = -np.matmul(new_camera.R.T, new_camera_center)\n",
    "\n",
    "    new_camera.world_view_transform = torch.tensor(getWorld2View2(new_camera.R, new_camera.T, new_camera.trans, new_camera.scale), \n",
    "                                                   device=new_camera.data_device, dtype=torch.float32).transpose(0, 1)\n",
    "    new_camera.full_proj_transform = (new_camera.world_view_transform.unsqueeze(0).bmm(new_camera.projection_matrix.unsqueeze(0))).squeeze(0)\n",
    "    new_camera.camera_center = new_camera.world_view_transform.inverse()[3, :3]\n",
    "    \n",
    "    for p in new_camera.parameters(): p.requires_grad_(False)\n",
    "    return new_camera\n",
    "\n",
    "def render_sets(dataset : ModelParams, iteration : int, pipeline : PipelineParams, scene, gaussians):\n",
    "    with torch.no_grad():\n",
    "        dataset.white_background = False\n",
    "        bg_color = [1,1,1] if dataset.white_background else [0, 0, 0]\n",
    "        background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "        camera = scene.getTrainCameras()[0]\n",
    "        rendering = render(camera, gaussians, pipeline, background)[\"render\"]\n",
    "        gt = camera.original_image[0:3, :, :]\n",
    "        print(\"PSNR: \", psnr(rendering, gt).mean().item())\n",
    "        plt.imshow(rendering.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.show()\n",
    "\n",
    "        distance, vector = camera_to_object(camera, gaussians)\n",
    "        forward_vector = camera_forward(camera)\n",
    "        dot = torch.clamp(torch.dot(forward_vector, vector), -1.0, 1.0)\n",
    "        angle_rad = torch.acos(dot)\n",
    "        import math\n",
    "        # angle_deg = angle_rad * 180.0 / math.pi\n",
    "        # print(f\"Angle between camera forward vector and object center of gravity vector: {angle_deg:.2f} degrees\")\n",
    "\n",
    "        # print(f\"Vector from camera to object center of gravity: {vector.tolist()}\")\n",
    "        # print(f\"Forward vector of camera: {forward_vector.tolist()}\")\n",
    "        # print(f\"Camera position: {camera.camera_center.tolist()}\")\n",
    "        print(f\"Distance from camera to object center of gravity: {distance:.4f}\")\n",
    "        step = -2\n",
    "        delta = ((forward_vector) * step).tolist()\n",
    "        new_camera = translate_camera(camera, delta=delta)\n",
    "        distance, vector = camera_to_object(new_camera, gaussians)\n",
    "        print(f\"New distance from camera to object center of gravity: {distance:.4f}\")\n",
    "        # print(f\"New camera position: {new_camera.camera_center.tolist()}\")\n",
    "        translated_rendering = render(new_camera, gaussians, pipeline, background)[\"render\"]\n",
    "        plt.imshow(translated_rendering.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.show()\n",
    "\n",
    "render_sets(dataset, iteration, pp, scene, gaussians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209af1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "from pathlib import Path\n",
    "import imageio.v3 as iio\n",
    "from scene.dataset_readers import CameraInfo\n",
    "from utils.graphics_utils import getWorld2View2, focal2fov, fov2focal\n",
    "from utils.camera_utils import camera_to_JSON, loadCam\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "data_root = '/home/wl757/multiplexed-pixels/plenoxels/blender_data/ficus'\n",
    "\n",
    "with open(os.path.join(data_root, 'transforms_train.json'), 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# choose one image to generate new views from\n",
    "indices = [0, 50, 59, 60, 70, 90, 2, 25, 38, 79, 90, 0, 11, 23, 27, 37]\n",
    "# indices = [0]\n",
    "\n",
    "# 4x4 grid of new views\n",
    "x_linspace = np.linspace(start = -0.5, stop = 0.5, num = 4)\n",
    "data_gen = copy.deepcopy(data)\n",
    "\n",
    "save_path = os.path.join(data_root, 'new_multiplexed_views')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "frames = []\n",
    "\n",
    "to8b = lambda x: (255 * torch.permute(x, (1, 2, 0)).clamp(0, 1).detach().cpu().numpy()).astype(\"uint8\")\n",
    "\n",
    "i = 0\n",
    "for index in indices:\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))    \n",
    "    axes = axes.flatten()\n",
    "    for x in x_linspace:\n",
    "        for y in x_linspace:\n",
    "            frame = copy.deepcopy(data['frames'][index])\n",
    "            cam_name = os.path.join(data_root, frame['file_path'] + \".png\")\n",
    "            c2w = np.array(frame['transform_matrix'])\n",
    "            camera_offset = np.array([x, y, 0])\n",
    "            world_offset = c2w[:3, :3] @ camera_offset\n",
    "            c2w[:3, 3] += world_offset\n",
    "            frame['transform_matrix'] = c2w.tolist()\n",
    "            frame['file_path'] = os.path.join(save_path, f'r_{index}_{i}')\n",
    "            c2w[:3, 1:3] *= -1\n",
    "            w2c = np.linalg.inv(c2w)\n",
    "            R = np.transpose(w2c[:3, :3])\n",
    "            T = w2c[:3, 3]\n",
    "            image_path = os.path.join(data_root, cam_name)\n",
    "            image_name = Path(cam_name).stem\n",
    "            image = Image.open(image_path)\n",
    "            width, height = image.size\n",
    "            fovx = data['camera_angle_x']\n",
    "            fovy = focal2fov(fov2focal(fovx, height), width)\n",
    "\n",
    "            camera = Camera(image.size, colmap_id=index, R=R, T=T, FoVx=fovx, FoVy=fovy,\n",
    "                            depth_params=None, image=image, invdepthmap=None, image_name=image_name, uid=i)\n",
    "            background = torch.tensor([0, 0, 0], dtype=torch.float32, device=\"cuda\")\n",
    "            rendering = render(camera, gaussians, pp, background)[\"render\"]\n",
    "            axes[i].imshow(rendering.permute(1, 2, 0).detach().cpu().numpy())\n",
    "            # iio.imwrite(os.path.join(save_path, f'r_{index}_{i}.png'), to8b(rendering))\n",
    "            axes[i].axis('off')\n",
    "            i += 1\n",
    "            frames.append(frame)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    i = 0\n",
    "\n",
    "\n",
    "data_gen['frames'] = frames\n",
    "with open(os.path.join(data_root, 'transforms_train_improvement.json'), 'w') as f:\n",
    "    json.dump(data_gen, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60850677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "from pathlib import Path\n",
    "import imageio.v3 as iio\n",
    "from scene.dataset_readers import CameraInfo\n",
    "from utils.graphics_utils import getWorld2View2, focal2fov, fov2focal\n",
    "from utils.camera_utils import camera_to_JSON, loadCam\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "data_root = '/home/wl757/multiplexed-pixels/plenoxels/blender_data/lego_gen12'\n",
    "\n",
    "with open(os.path.join(data_root, 'transforms_train_gaussian_splatting_multiviews.json'), 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# choose one image to generate new views from\n",
    "indices = [59]\n",
    "\n",
    "# 4x4 grid of new views\n",
    "num_views = 16\n",
    "to8b = lambda x: (255 * torch.permute(x, (1, 2, 0)).clamp(0, 1).detach().cpu().numpy()).astype(\"uint8\")\n",
    "frames = {Path(frame[\"file_path\"]).stem: frame for frame in data['frames']}\n",
    "\n",
    "i = 0\n",
    "for index in indices:\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))    \n",
    "    axes = axes.flatten()\n",
    "    for view_id in range(num_views):\n",
    "        frame = frames[f'r_{index}_{view_id}']\n",
    "        cam_name = os.path.join(data_root, frame['file_path'] + \".png\")\n",
    "        c2w = np.array(frame['transform_matrix'])\n",
    "        c2w[:3, 1:3] *= -1\n",
    "        w2c = np.linalg.inv(c2w)\n",
    "        R = np.transpose(w2c[:3, :3])\n",
    "        T = w2c[:3, 3]\n",
    "        image_path = os.path.join(data_root, cam_name)\n",
    "        image_name = Path(cam_name).stem\n",
    "        image = Image.open(image_path)\n",
    "        width, height = image.size\n",
    "        fovx = data['camera_angle_x']\n",
    "        fovy = focal2fov(fov2focal(fovx, height), width)\n",
    "\n",
    "        camera = Camera(image.size, colmap_id=index, R=R, T=T, FoVx=fovx, FoVy=fovy,\n",
    "                        depth_params=None, image=image, invdepthmap=None, image_name=image_name, uid=i)\n",
    "        background = torch.tensor([0, 0, 0], dtype=torch.float32, device=\"cuda\")\n",
    "        rendering = render(camera, gaussians, pp, background)[\"render\"]\n",
    "        axes[i].imshow(rendering.permute(1, 2, 0).detach().cpu().numpy())\n",
    "        axes[i].axis('off')\n",
    "        i += 1\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d2ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "from pathlib import Path\n",
    "import imageio.v3 as iio\n",
    "from scene.dataset_readers import CameraInfo\n",
    "from utils.graphics_utils import getWorld2View2, focal2fov, fov2focal\n",
    "from utils.camera_utils import camera_to_JSON, loadCam\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "data_root = '/home/wl757/multiplexed-pixels/plenoxels/blender_data/lego_gen12'\n",
    "\n",
    "with open(os.path.join(data_root, 'transforms_train.json'), 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# choose one image to generate new views from\n",
    "indices = [0, 50, 59]\n",
    "\n",
    "# 4x4 grid of new views\n",
    "x_linspace = np.linspace(start = -0.5, stop = 0.5, num = 4)\n",
    "data_gen = copy.deepcopy(data)\n",
    "\n",
    "# old code with incorrect multiplexing\n",
    "i = 0\n",
    "for index in indices:\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))    \n",
    "    axes = axes.flatten()\n",
    "    for x in x_linspace:\n",
    "        for y in x_linspace:\n",
    "            frame = copy.deepcopy(data['frames'][index])\n",
    "            cam_name = os.path.join(data_root, frame['file_path'] + \".png\")\n",
    "            c2w = np.array(frame['transform_matrix'])\n",
    "            c2w[0, -1] += x\n",
    "            c2w[1, -1] += y\n",
    "            c2w[:3, 1:3] *= -1\n",
    "            w2c = np.linalg.inv(c2w)\n",
    "            R = np.transpose(w2c[:3, :3])\n",
    "            T = w2c[:3, 3]\n",
    "            image_path = os.path.join(data_root, cam_name)\n",
    "            image_name = Path(cam_name).stem\n",
    "            image = Image.open(image_path)\n",
    "            width, height = image.size\n",
    "            fovx = data['camera_angle_x']\n",
    "            fovy = focal2fov(fov2focal(fovx, height), width)\n",
    "\n",
    "            camera = Camera(image.size, colmap_id=index, R=R, T=T, FoVx=fovx, FoVy=fovy,\n",
    "                            depth_params=None, image=image, invdepthmap=None, image_name=image_name, uid=i)\n",
    "            background = torch.tensor([0, 0, 0], dtype=torch.float32, device=\"cuda\")\n",
    "            rendering = render(camera, gaussians, pp, background)[\"render\"]\n",
    "\n",
    "            axes[i].imshow(rendering.permute(1, 2, 0).detach().cpu().numpy())\n",
    "\n",
    "            axes[i].axis('off')\n",
    "            i += 1\n",
    "    i = 0\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#     frames = []\n",
    "#     cnt = 0\n",
    "#     for i in x_linspace:\n",
    "#         for j in x_linspace:\n",
    "#             sample = copy.deepcopy(view_sample)\n",
    "#             sample['file_path'] = f'./render_5_views/r_{index}_{cnt}'\n",
    "#             cnt += 1\n",
    "#             sample['transform_matrix'][0][-1] += i\n",
    "#             sample['transform_matrix'][1][-1] += j\n",
    "# #             print(i,j, sample['transform_matrix'][0][-1],sample['transform_matrix'][1][-1] )\n",
    "#             frames.append(sample)\n",
    "\n",
    "#     data_gen['frames'] = frames\n",
    "#     # json.dumps(frames)\n",
    "#     file.close()\n",
    "\n",
    "    # with open(f'/home/wl757/multiplexed-pixels/plenoxels/blender_data/hotdog/transforms_train_gaussian_splatting.json','w') as f:\n",
    "    #     json.dump(data_gen, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaussian_splatting",
   "language": "python",
   "name": "gaussian_splatting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
